{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":9701926,"sourceType":"datasetVersion","datasetId":5932994},{"sourceId":9702032,"sourceType":"datasetVersion","datasetId":5933084}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-24T06:31:03.432598Z","iopub.execute_input":"2024-10-24T06:31:03.432912Z","iopub.status.idle":"2024-10-24T06:31:04.575534Z","shell.execute_reply.started":"2024-10-24T06:31:03.432870Z","shell.execute_reply":"2024-10-24T06:31:04.574546Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/resume-dataset/Resumes.json\n/kaggle/input/vocab-bert/vocab/vocab.txt\n","output_type":"stream"}]},{"cell_type":"code","source":"import os\nvocab_path ='/kaggle/input/vocab-bert/vocab/vocab.txt'\nif not os.path.isfile(vocab_path):\n    print(f\"Vocabulary file not found at {vocab_path}\")","metadata":{"execution":{"iopub.status.busy":"2024-10-24T06:31:04.577378Z","iopub.execute_input":"2024-10-24T06:31:04.577812Z","iopub.status.idle":"2024-10-24T06:31:04.582959Z","shell.execute_reply.started":"2024-10-24T06:31:04.577774Z","shell.execute_reply":"2024-10-24T06:31:04.581948Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"!pip install seqeval","metadata":{"execution":{"iopub.status.busy":"2024-10-24T06:31:04.584138Z","iopub.execute_input":"2024-10-24T06:31:04.584450Z","iopub.status.idle":"2024-10-24T06:31:19.988666Z","shell.execute_reply.started":"2024-10-24T06:31:04.584417Z","shell.execute_reply":"2024-10-24T06:31:19.987633Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Collecting seqeval\n  Downloading seqeval-1.2.2.tar.gz (43 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.26.4)\nRequirement already satisfied: scikit-learn>=0.21.3 in /opt/conda/lib/python3.10/site-packages (from seqeval) (1.2.2)\nRequirement already satisfied: scipy>=1.3.2 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.14.1)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (1.4.2)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn>=0.21.3->seqeval) (3.5.0)\nBuilding wheels for collected packages: seqeval\n  Building wheel for seqeval (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16161 sha256=ed5e763fba8d0ee3fedc9236cf68b0a3a8bc084e4b9e8292028fb70397e19cc4\n  Stored in directory: /root/.cache/pip/wheels/1a/67/4a/ad4082dd7dfc30f2abfe4d80a2ed5926a506eb8a972b4767fa\nSuccessfully built seqeval\nInstalling collected packages: seqeval\nSuccessfully installed seqeval-1.2.2\n","output_type":"stream"}]},{"cell_type":"code","source":"pip show seqeval","metadata":{"execution":{"iopub.status.busy":"2024-10-24T06:31:19.990111Z","iopub.execute_input":"2024-10-24T06:31:19.990478Z","iopub.status.idle":"2024-10-24T06:31:30.549265Z","shell.execute_reply.started":"2024-10-24T06:31:19.990436Z","shell.execute_reply":"2024-10-24T06:31:30.547937Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Name: seqeval\nVersion: 1.2.2\nSummary: Testing framework for sequence labeling\nHome-page: https://github.com/chakki-works/seqeval\nAuthor: Hironsan\nAuthor-email: hiroki.nakayama.py@gmail.com\nLicense: MIT\nLocation: /opt/conda/lib/python3.10/site-packages\nRequires: numpy, scikit-learn\nRequired-by: \nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}]},{"cell_type":"code","source":"import warnings\n\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='seqeval')","metadata":{"execution":{"iopub.status.busy":"2024-10-24T06:31:30.553456Z","iopub.execute_input":"2024-10-24T06:31:30.553899Z","iopub.status.idle":"2024-10-24T06:31:30.559297Z","shell.execute_reply.started":"2024-10-24T06:31:30.553862Z","shell.execute_reply":"2024-10-24T06:31:30.558222Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"import re\nimport json\nimport logging\nimport numpy as np\nfrom tqdm import trange\nimport torch\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom seqeval.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","metadata":{"execution":{"iopub.status.busy":"2024-10-24T06:31:30.560620Z","iopub.execute_input":"2024-10-24T06:31:30.560982Z","iopub.status.idle":"2024-10-24T06:31:34.099688Z","shell.execute_reply.started":"2024-10-24T06:31:30.560946Z","shell.execute_reply":"2024-10-24T06:31:34.098839Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"def convert_dataset(filePath):\n    \"\"\"Convert the dataset in json format to the required text and entity format\"\"\"\n    try:\n        training_data = []\n        lines = []\n        # Open the file using UTF-8 encoding\n        with open(filePath, 'r', encoding='utf-8') as f:\n            data = json.load(f)  # load the JSON content\n            print(type(data))\n            for item in data:\n                lines.append(json.dumps(item))\n            \n\n        for line in lines:\n            data = json.loads(line)\n            text = data['content'].replace(\"\\n\", \" \")\n            entities = []\n            data_annotations = data['annotation']\n            if data_annotations is not None:\n                for annotation in data_annotations:\n                    point = annotation['points'][0]\n                    labels = annotation['label']\n                    if not isinstance(labels, list):\n                        labels = [labels]\n\n                    for label in labels:\n                        point_start = point['start']\n                        point_end = point['end']\n                        point_text = point['text']\n\n                        lstrip_diff = len(point_text) - len(point_text.lstrip())\n                        rstrip_diff = len(point_text) - len(point_text.rstrip())\n                        if lstrip_diff != 0:\n                            point_start = point_start + lstrip_diff\n                        if rstrip_diff != 0:\n                            point_end = point_end - rstrip_diff\n                        entities.append((point_start, point_end + 1, label))\n            training_data.append((text, {\"entities\": entities}))\n        return training_data\n    except Exception as e:\n        logging.exception(\"Unable to process \" +\n                          filePath + \"\\n\" + \"error = \" + str(e))\n        return None","metadata":{"execution":{"iopub.status.busy":"2024-10-24T06:31:34.100923Z","iopub.execute_input":"2024-10-24T06:31:34.101395Z","iopub.status.idle":"2024-10-24T06:31:34.111920Z","shell.execute_reply.started":"2024-10-24T06:31:34.101360Z","shell.execute_reply":"2024-10-24T06:31:34.111030Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"def trim_entity_spans(data: list) -> list:\n    \"\"\"Removes leading and trailing white spaces from entity spans.\n        The data in spaCy json format is cleaned\n    \"\"\"\n    invalid_span_tokens = re.compile(r'\\s')\n\n    cleaned_data = []\n    for text, annotations in data:\n        entities = annotations['entities']\n        valid_entities = []\n        for start, end, label in entities:\n            valid_start = start\n            valid_end = end\n            while valid_start < len(text) and invalid_span_tokens.match(\n                    text[valid_start]):\n                valid_start += 1\n            while valid_end > 1 and invalid_span_tokens.match(\n                    text[valid_end - 1]):\n                valid_end -= 1\n            valid_entities.append([valid_start, valid_end, label])\n        cleaned_data.append([text, {'entities': valid_entities}])\n    return cleaned_data","metadata":{"execution":{"iopub.status.busy":"2024-10-24T06:31:34.113158Z","iopub.execute_input":"2024-10-24T06:31:34.113997Z","iopub.status.idle":"2024-10-24T06:31:34.134613Z","shell.execute_reply.started":"2024-10-24T06:31:34.113939Z","shell.execute_reply":"2024-10-24T06:31:34.133693Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"def get_label(offset, labels):\n    \"\"\"The function compares the offset of the token \n    to the character offsets of the labeled spans \n    in labels to determine if the token falls within any labeled span.\"\"\"\n    if offset[0] == 0 and offset[1] == 0:\n        return 'O'\n    for label in labels:\n        if offset[1] >= label[0] and offset[0] <= label[1]:\n            return label[2]\n    return 'O'","metadata":{"execution":{"iopub.status.busy":"2024-10-24T06:31:34.135902Z","iopub.execute_input":"2024-10-24T06:31:34.136413Z","iopub.status.idle":"2024-10-24T06:31:34.148151Z","shell.execute_reply.started":"2024-10-24T06:31:34.136366Z","shell.execute_reply":"2024-10-24T06:31:34.147363Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"tags_vals = [\"UNKNOWN\", \"O\", \"Name\", \"Degree\", \"Skills\", \"College Name\", \"Email Address\",\n             \"Designation\", \"Companies worked at\", \"Graduation Year\", \"Years of Experience\", \"Location\"]\n\ntag2idx = {t: i for i, t in enumerate(tags_vals)}\nidx2tag = {i: t for i, t in enumerate(tags_vals)}","metadata":{"execution":{"iopub.status.busy":"2024-10-24T06:31:34.149317Z","iopub.execute_input":"2024-10-24T06:31:34.149668Z","iopub.status.idle":"2024-10-24T06:31:34.158458Z","shell.execute_reply.started":"2024-10-24T06:31:34.149635Z","shell.execute_reply":"2024-10-24T06:31:34.157447Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"print(tags_vals)\nprint(tag2idx)\nprint(idx2tag)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T06:31:34.159634Z","iopub.execute_input":"2024-10-24T06:31:34.159916Z","iopub.status.idle":"2024-10-24T06:31:34.169486Z","shell.execute_reply.started":"2024-10-24T06:31:34.159886Z","shell.execute_reply":"2024-10-24T06:31:34.168581Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"['UNKNOWN', 'O', 'Name', 'Degree', 'Skills', 'College Name', 'Email Address', 'Designation', 'Companies worked at', 'Graduation Year', 'Years of Experience', 'Location']\n{'UNKNOWN': 0, 'O': 1, 'Name': 2, 'Degree': 3, 'Skills': 4, 'College Name': 5, 'Email Address': 6, 'Designation': 7, 'Companies worked at': 8, 'Graduation Year': 9, 'Years of Experience': 10, 'Location': 11}\n{0: 'UNKNOWN', 1: 'O', 2: 'Name', 3: 'Degree', 4: 'Skills', 5: 'College Name', 6: 'Email Address', 7: 'Designation', 8: 'Companies worked at', 9: 'Graduation Year', 10: 'Years of Experience', 11: 'Location'}\n","output_type":"stream"}]},{"cell_type":"code","source":"def process_resume(data, tokenizer, tag2idx, max_len, is_test=False):\n    #data[0] is the text of the resume\n    tok = tokenizer.encode_plus(\n        data[0],\n        max_length=max_len,\n        padding ='max_length',\n        truncation=True,\n        return_tensors=\"pt\",\n        return_offsets_mapping=True)\n    \n    curr_sent = {'orig_labels': [], 'labels': []}\n\n    padding_length = max_len - tok['input_ids'].shape[1]\n\n    if not is_test:\n        labels = data[1]['entities']\n        labels.reverse()\n        for off in tok['offset_mapping'][0]:\n            label = get_label(off, labels)\n            curr_sent['orig_labels'].append(label)\n            curr_sent['labels'].append(tag2idx[label])\n\n        curr_sent['labels'] = curr_sent['labels'] + ([0] * padding_length)\n\n    curr_sent['input_ids'] = tok['input_ids'].squeeze(0).tolist()  + ([0] * padding_length)\n    curr_sent['token_type_ids'] = tok['token_type_ids'].squeeze(0).tolist()  + ([0] * padding_length)\n    curr_sent['attention_mask'] = tok['attention_mask'].squeeze(0).tolist()  + ([0] * padding_length)\n    \n    if 'token_type_ids' in tok:\n        curr_sent['token_type_ids'] = tok['token_type_ids'].squeeze(0).tolist() + ([0] * padding_length)\n    else:\n        curr_sent['token_type_ids'] = [0] * max_len\n        \n    \n#     print(curr_sent)\n\n    return curr_sent","metadata":{"execution":{"iopub.status.busy":"2024-10-24T06:31:34.170667Z","iopub.execute_input":"2024-10-24T06:31:34.171058Z","iopub.status.idle":"2024-10-24T06:31:34.181214Z","shell.execute_reply.started":"2024-10-24T06:31:34.171013Z","shell.execute_reply":"2024-10-24T06:31:34.180400Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"class ResumeDataset(Dataset):\n    def __init__(self, resume, tokenizer, tag2idx, max_len, is_test=False):\n        self.resume = resume\n        self.tokenizer = tokenizer\n        self.is_test = is_test\n        self.tag2idx = tag2idx\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.resume)\n\n    def __getitem__(self, idx):\n        data = process_resume(\n            self.resume[idx], self.tokenizer, self.tag2idx, self.max_len, self.is_test)\n        return {\n            'input_ids': torch.tensor(data['input_ids'], dtype=torch.long),\n            'token_type_ids': torch.tensor(data['token_type_ids'], dtype=torch.long),\n            'attention_mask': torch.tensor(data['attention_mask'], dtype=torch.long),\n            'labels': torch.tensor(data['labels'], dtype=torch.long),\n            'orig_label': data['orig_labels']\n        }","metadata":{"execution":{"iopub.status.busy":"2024-10-24T06:31:34.182310Z","iopub.execute_input":"2024-10-24T06:31:34.182629Z","iopub.status.idle":"2024-10-24T06:31:34.194800Z","shell.execute_reply.started":"2024-10-24T06:31:34.182588Z","shell.execute_reply":"2024-10-24T06:31:34.193816Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def get_hyperparameters(model, ff):\n\n    # ff: full_finetuning\n    if ff:\n        param_optimizer = list(model.named_parameters())\n        no_decay = [\"bias\", \"gamma\", \"beta\"]\n        optimizer_grouped_parameters = [\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if not any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay_rate\": 0.01,\n            },\n            {\n                \"params\": [\n                    p for n, p in param_optimizer if any(nd in n for nd in no_decay)\n                ],\n                \"weight_decay_rate\": 0.0,\n            },\n        ]\n    else:\n        param_optimizer = list(model.classifier.named_parameters())\n        optimizer_grouped_parameters = [\n            {\"params\": [p for n, p in param_optimizer]}]\n\n    return optimizer_grouped_parameters","metadata":{"execution":{"iopub.status.busy":"2024-10-24T06:31:34.198971Z","iopub.execute_input":"2024-10-24T06:31:34.199383Z","iopub.status.idle":"2024-10-24T06:31:34.208867Z","shell.execute_reply.started":"2024-10-24T06:31:34.199348Z","shell.execute_reply":"2024-10-24T06:31:34.207952Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def get_special_tokens(tokenizer, tag2idx):\n    vocab = tokenizer.get_vocab()\n    pad_tok = vocab[\"[PAD]\"]\n    sep_tok = vocab[\"[SEP]\"]\n    cls_tok = vocab[\"[CLS]\"]\n    o_lab = tag2idx[\"O\"]\n\n    return pad_tok, sep_tok, cls_tok, o_lab\n\n\ndef annot_confusion_matrix(valid_tags, pred_tags):\n    \"\"\"\n    Create an annotated confusion matrix by adding label\n    annotations and formatting to sklearn's `confusion_matrix`.\n    \"\"\"\n\n    flat_valid_tags = [item for sublist in valid_tags for item in sublist]\n    flat_pred_tags = [item for sublist in pred_tags for item in sublist]\n\n    # Get the unique labels\n    header = sorted(list(set(flat_valid_tags + flat_pred_tags)))\n\n    # Compute the confusion matrix\n    matrix = confusion_matrix(flat_valid_tags, flat_pred_tags, labels=header)\n\n    # Format the matrix for printing\n    mat_formatted = [header[i] + \"\\t\\t\\t\" +\n                     str(row) for i, row in enumerate(matrix)]\n    content = \"\\t\" + \" \".join(header) + \"\\n\" + \"\\n\".join(mat_formatted)\n\n    return content\n\n\ndef flat_accuracy(valid_tags, pred_tags):\n    return (np.array(valid_tags) == np.array(pred_tags)).mean()","metadata":{"execution":{"iopub.status.busy":"2024-10-24T06:31:34.210072Z","iopub.execute_input":"2024-10-24T06:31:34.210519Z","iopub.status.idle":"2024-10-24T06:31:34.220284Z","shell.execute_reply.started":"2024-10-24T06:31:34.210473Z","shell.execute_reply":"2024-10-24T06:31:34.219498Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"def train_and_val_model(\n    model,\n    tokenizer,\n    optimizer,\n    epochs,\n    idx2tag,\n    tag2idx,\n    max_grad_norm,\n    device,\n    train_dataloader,\n    valid_dataloader\n):\n\n    pad_tok, sep_tok, cls_tok, o_lab = get_special_tokens(tokenizer, tag2idx)\n\n    epoch = 0\n    for _ in trange(epochs, desc=\"Epoch\"):\n        epoch += 1\n\n        # Training loop\n        print(\"Starting training loop.\")\n        model.train()\n        tr_loss, tr_accuracy = 0, 0\n        nb_tr_examples, nb_tr_steps = 0, 0\n        tr_preds, tr_labels = [], []\n\n        for step, batch in enumerate(train_dataloader):\n            # Add batch to gpu\n\n            # batch = tuple(t.to(device) for t in batch)\n            b_input_ids, b_input_mask, b_labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n            b_input_ids, b_input_mask, b_labels = b_input_ids.to(\n                device), b_input_mask.to(device), b_labels.to(device)\n\n            # Forward pass\n            outputs = model(\n                b_input_ids,\n                token_type_ids=None,\n                attention_mask=b_input_mask,\n                labels=b_labels,\n            )\n            loss, tr_logits = outputs[:2]\n\n            # Backward pass\n            loss.backward()\n\n            # Compute train loss\n            tr_loss += loss.item()\n            nb_tr_examples += b_input_ids.size(0)\n            nb_tr_steps += 1\n\n            # Subset out unwanted predictions on CLS/PAD/SEP tokens\n            preds_mask = (\n                (b_input_ids != cls_tok)\n                & (b_input_ids != pad_tok)\n                & (b_input_ids != sep_tok)\n            )\n\n            tr_logits = tr_logits.cpu().detach().numpy()\n            tr_label_ids = torch.masked_select(b_labels, (preds_mask == 1))\n            preds_mask = preds_mask.cpu().detach().numpy()\n            tr_batch_preds = np.argmax(tr_logits[preds_mask.squeeze()], axis=1)\n            tr_batch_labels = tr_label_ids.to(\"cpu\").numpy()\n            tr_preds.extend(tr_batch_preds)\n            tr_labels.extend(tr_batch_labels)\n\n            # Compute training accuracy\n            tmp_tr_accuracy = flat_accuracy(tr_batch_labels, tr_batch_preds)\n            tr_accuracy += tmp_tr_accuracy\n\n            # Gradient clipping\n            torch.nn.utils.clip_grad_norm_(\n                parameters=model.parameters(), max_norm=max_grad_norm\n            )\n\n            # Update parameters\n            optimizer.step()\n            model.zero_grad()\n\n        tr_loss = tr_loss / nb_tr_steps\n        tr_accuracy = tr_accuracy / nb_tr_steps\n\n        # Print training loss and accuracy per epoch\n        print(f\"Train loss: {tr_loss}\")\n        print(f\"Train accuracy: {tr_accuracy}\")\n\n        \"\"\"\n        Validation loop\n        \"\"\"\n        print(\"Starting validation loop.\")\n\n        pred_tags = []\n        valid_tags = []\n        eval_loss, eval_accuracy = 0, 0  \n        nb_eval_steps, nb_eval_examples = 0, 0\n        predictions, true_labels = [], []\n        \n        for i, batch in enumerate(valid_dataloader):\n            b_input_ids, b_input_mask, b_labels = batch['input_ids'], batch['attention_mask'], batch['labels']\n            b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n        \n            with torch.no_grad():\n                outputs = model(\n                    b_input_ids,\n                    token_type_ids=None,\n                    attention_mask=b_input_mask,\n                    labels=b_labels,\n                )\n                tmp_eval_loss, logits = outputs[:2]\n            \n                preds_mask = (\n                    (b_input_ids != cls_tok)\n                    & (b_input_ids != pad_tok)\n                    & (b_input_ids != sep_tok)\n                )\n                preds_mask = preds_mask.cpu().detach()\n            \n                # Keep logits as a tensor for any operation requiring it\n                logits = logits.cpu().detach()\n            \n                # Use preds_mask with torch.masked_select\n                label_ids = torch.masked_select(b_labels.cpu(), preds_mask)\n                val_batch_preds = np.argmax(logits[preds_mask].numpy(), axis=1)  # Convert logits to numpy here, after masking\n                val_batch_labels = label_ids.to(\"cpu\").numpy()\n                predictions.extend(val_batch_preds)\n                true_labels.extend(val_batch_labels)\n            \n                eval_loss += tmp_eval_loss.item()\n            \n                tmp_eval_accuracy = flat_accuracy(\n                    val_batch_labels, val_batch_preds)\n                eval_accuracy += tmp_eval_accuracy\n            \n                nb_eval_examples += b_input_ids.size(0)\n                nb_eval_steps += 1\n            \n                    \n            # Convert logits and labels to numpy\n            logits = logits.detach().cpu().numpy()\n            label_ids = b_labels.to('cpu').numpy()\n            input_ids = b_input_ids.cpu().numpy()\n        \n            for i in range(len(label_ids)):\n                # Apply mask to remove special tokens\n                seq_pred = []\n                seq_true = []\n                for j in range(len(label_ids[i])):\n                    if input_ids[i][j] not in [cls_tok, pad_tok, sep_tok]:  \n                        seq_pred.append(idx2tag[np.argmax(logits[i][j])])  \n                        seq_true.append(idx2tag[label_ids[i][j]])  \n                \n                # Append sequence-level predictions and true labels\n                pred_tags.append(seq_pred)\n                valid_tags.append(seq_true)\n        \n        # Now pred_tags and valid_tags are lists of lists (one sublist per sequence)\n        cl_report = classification_report(valid_tags, pred_tags)\n        conf_mat = annot_confusion_matrix(valid_tags, pred_tags)\n        \n        # Report metrics\n        eval_loss = eval_loss / nb_eval_steps\n        eval_accuracy = flat_accuracy([item for sublist in valid_tags for item in sublist],\n                                      [item for sublist in pred_tags for item in sublist])\n        \n        print(f\"Validation loss: {eval_loss}\")\n        print(f\"Validation Accuracy: {eval_accuracy}\")\n        print(f\"Classification Report:\\n {cl_report}\")\n        print(f\"Confusion Matrix:\\n {conf_mat}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T06:31:34.221528Z","iopub.execute_input":"2024-10-24T06:31:34.221876Z","iopub.status.idle":"2024-10-24T06:31:34.245566Z","shell.execute_reply.started":"2024-10-24T06:31:34.221835Z","shell.execute_reply":"2024-10-24T06:31:34.244649Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch\nfrom transformers import BertForTokenClassification, BertTokenizerFast\nfrom torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\nfrom torch.optim import Adam\n\noutput_path = '/kaggle/working'\n\nMAX_LEN = 500\nEPOCHS = 5\nMAX_GRAD_NORM = 1.0\nMODEL_NAME = 'bert-base-uncased'\nTOKENIZER = BertTokenizerFast.from_pretrained(MODEL_NAME, do_lower_case=True)\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# data = trim_entity_spans(convert_goldparse('data/Resumes.json'))\ndata = convert_dataset('/kaggle/input/resume-dataset/Resumes.json')\nprint(data[0])\n\n# print(len(data))\n# print(data)\nif data is not None:\n    data = trim_entity_spans(data)\n    print(data[0])\nelse:\n    print(\"No data to process due to earlier errors.\")\n\ntotal = len(data)\n# print(f\"Total resumes: {total}\")\ntrain_data, val_data = data[:180],data[180:]\n\nprint(\"Creating the datasets...\")\ntrain_d = ResumeDataset(train_data, TOKENIZER, tag2idx, MAX_LEN)\nprint(train_d[0])\nval_d = ResumeDataset(val_data, TOKENIZER, tag2idx, MAX_LEN)\n\ntrain_sampler = RandomSampler(train_d)\ntrain_dl = DataLoader(train_d, sampler=train_sampler, batch_size=8)\n\nval_dl = DataLoader(val_d, batch_size=4)\n\nmodel = BertForTokenClassification.from_pretrained(\n    MODEL_NAME, num_labels=len(tag2idx))\nmodel.to(DEVICE)\noptimizer_grouped_parameters = get_hyperparameters(model, True)\noptimizer = Adam(optimizer_grouped_parameters, lr=3e-5)\n\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T06:31:34.247216Z","iopub.execute_input":"2024-10-24T06:31:34.247643Z","iopub.status.idle":"2024-10-24T06:31:42.384728Z","shell.execute_reply.started":"2024-10-24T06:31:34.247599Z","shell.execute_reply":"2024-10-24T06:31:42.383707Z"},"trusted":true},"execution_count":17,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a060931bf2c641268494c7ec859e2374"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e6281b767f14f49a948a370b6b5df4c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"74b018e6b56242a1a2c92c76d21c4a33"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"60f5042d23f64f05992f44e49dd11762"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"<class 'list'>\n(\"Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  • To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and company's growth in best possible ways.  Willing to relocate to: Bangalore, Karnataka  WORK EXPERIENCE  Application Development Associate  Accenture -  November 2017 to Present  Role: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input. Also, Training the bot for different possible utterances (Both positive and negative), which will be given as input by the user.  EDUCATION  B.E in Information science and engineering  B.v.b college of engineering and technology -  Hubli, Karnataka  August 2013 to June 2017  12th in Mathematics  Woodbine modern school  April 2011 to March 2013  10th  Kendriya Vidyalaya  April 2001 to March 2011  SKILLS  C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year), Database Management System (Less than 1 year), Java (Less than 1 year)  ADDITIONAL INFORMATION  Technical Skills  https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN   • Programming language: C, C++, Java • Oracle PeopleSoft • Internet Of Things • Machine Learning • Database Management System • Computer Networks • Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  • Honest and Hard-Working • Tolerant and Flexible to Different Situations • Polite and Calm • Team-Player\", {'entities': [(1296, 1622, 'Skills'), (993, 1154, 'Skills'), (939, 957, 'College Name'), (883, 905, 'College Name'), (856, 860, 'Graduation Year'), (771, 814, 'College Name'), (727, 769, 'Designation'), (407, 416, 'Companies worked at'), (372, 405, 'Designation'), (95, 145, 'Email Address'), (60, 69, 'Location'), (49, 58, 'Companies worked at'), (13, 46, 'Designation'), (0, 12, 'Name')]})\n[\"Abhishek Jha Application Development Associate - Accenture  Bengaluru, Karnataka - Email me on Indeed: indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a  • To work for an organization which provides me the opportunity to improve my skills and knowledge for my individual and company's growth in best possible ways.  Willing to relocate to: Bangalore, Karnataka  WORK EXPERIENCE  Application Development Associate  Accenture -  November 2017 to Present  Role: Currently working on Chat-bot. Developing Backend Oracle PeopleSoft Queries for the Bot which will be triggered based on given input. Also, Training the bot for different possible utterances (Both positive and negative), which will be given as input by the user.  EDUCATION  B.E in Information science and engineering  B.v.b college of engineering and technology -  Hubli, Karnataka  August 2013 to June 2017  12th in Mathematics  Woodbine modern school  April 2011 to March 2013  10th  Kendriya Vidyalaya  April 2001 to March 2011  SKILLS  C (Less than 1 year), Database (Less than 1 year), Database Management (Less than 1 year), Database Management System (Less than 1 year), Java (Less than 1 year)  ADDITIONAL INFORMATION  Technical Skills  https://www.indeed.com/r/Abhishek-Jha/10e7a8cb732bc43a?isid=rex-download&ikw=download-top&co=IN   • Programming language: C, C++, Java • Oracle PeopleSoft • Internet Of Things • Machine Learning • Database Management System • Computer Networks • Operating System worked on: Linux, Windows, Mac  Non - Technical Skills  • Honest and Hard-Working • Tolerant and Flexible to Different Situations • Polite and Calm • Team-Player\", {'entities': [[1296, 1622, 'Skills'], [993, 1154, 'Skills'], [939, 957, 'College Name'], [883, 905, 'College Name'], [856, 860, 'Graduation Year'], [771, 814, 'College Name'], [727, 769, 'Designation'], [407, 416, 'Companies worked at'], [372, 405, 'Designation'], [95, 145, 'Email Address'], [60, 69, 'Location'], [49, 58, 'Companies worked at'], [13, 46, 'Designation'], [0, 12, 'Name']]}]\nCreating the datasets...\n{'input_ids': tensor([  101, 11113, 24158,  5369,  2243,  1046,  3270,  4646,  2458,  5482,\n         1011,  9669,  5397,  8191, 14129,  1010, 12092,  1011, 10373,  2033,\n         2006,  5262,  1024,  5262,  1012,  4012,  1013,  1054,  1013, 11113,\n        24158,  5369,  2243,  1011,  1046,  3270,  1013,  2184,  2063,  2581,\n         2050,  2620, 27421,  2581, 16703,  9818, 23777,  2050,  1528,  2000,\n         2147,  2005,  2019,  3029,  2029,  3640,  2033,  1996,  4495,  2000,\n         5335,  2026,  4813,  1998,  3716,  2005,  2026,  3265,  1998,  2194,\n         1005,  1055,  3930,  1999,  2190,  2825,  3971,  1012,  5627,  2000,\n        20102,  2000,  1024, 14022,  1010, 12092,  2147,  3325,  4646,  2458,\n         5482,  9669,  5397,  1011,  2281,  2418,  2000,  2556,  2535,  1024,\n         2747,  2551,  2006, 11834,  1011, 28516,  1012,  4975,  2067, 10497,\n        14721,  7243, 15794, 10861,  5134,  2005,  1996, 28516,  2029,  2097,\n         2022, 13330,  2241,  2006,  2445,  7953,  1012,  2036,  1010,  2731,\n         1996, 28516,  2005,  2367,  2825, 14395, 26755,  1006,  2119,  3893,\n         1998,  4997,  1007,  1010,  2029,  2097,  2022,  2445,  2004,  7953,\n         2011,  1996,  5310,  1012,  2495,  1038,  1012,  1041,  1999,  2592,\n         2671,  1998,  3330,  1038,  1012,  1058,  1012,  1038,  2267,  1997,\n         3330,  1998,  2974,  1011,  9594,  3669,  1010, 12092,  2257,  2286,\n         2000,  2238,  2418,  5940,  1999,  5597,  3536, 16765,  2715,  2082,\n         2258,  2249,  2000,  2233,  2286,  6049,  6358, 13626,  8717,  6819,\n        25838, 22923,  2258,  2541,  2000,  2233,  2249,  4813,  1039,  1006,\n         2625,  2084,  1015,  2095,  1007,  1010,  7809,  1006,  2625,  2084,\n         1015,  2095,  1007,  1010,  7809,  2968,  1006,  2625,  2084,  1015,\n         2095,  1007,  1010,  7809,  2968,  2291,  1006,  2625,  2084,  1015,\n         2095,  1007,  1010,  9262,  1006,  2625,  2084,  1015,  2095,  1007,\n         3176,  2592,  4087,  4813, 16770,  1024,  1013,  1013,  7479,  1012,\n         5262,  1012,  4012,  1013,  1054,  1013, 11113, 24158,  5369,  2243,\n         1011,  1046,  3270,  1013,  2184,  2063,  2581,  2050,  2620, 27421,\n         2581, 16703,  9818, 23777,  2050,  1029,  2003,  3593,  1027, 10151,\n         1011,  8816,  1004, 20912,  2860,  1027,  8816,  1011,  2327,  1004,\n         2522,  1027,  1999,  1528,  4730,  2653,  1024,  1039,  1010,  1039,\n         1009,  1009,  1010,  9262,  1528, 14721,  7243, 15794,  1528,  4274,\n         1997,  2477,  1528,  3698,  4083,  1528,  7809,  2968,  2291,  1528,\n         3274,  6125,  1528,  4082,  2291,  2499,  2006,  1024, 11603,  1010,\n         3645,  1010,  6097,  2512,  1011,  4087,  4813,  1528,  7481,  1998,\n         2524,  1011,  2551,  1528, 23691,  1998, 12379,  2000,  2367,  8146,\n         1528, 13205,  1998,  5475,  1528,  2136,  1011,  2447,   102,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), 'labels': tensor([ 1,  2,  2,  2,  2,  2,  2,  7,  7,  7,  1,  8,  8, 11, 11, 11,  1,  1,\n         1,  1,  1,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,\n         6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  6,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  7,  7,\n         7,  8,  8,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  7,  7,  7,  7,  7,  7,  7,\n         7,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  9,  1,  1,  1,  5,  5,  5,  5,  1,  1,  1,  1,  1,  1,  5,  5,\n         5,  5,  5,  5,  1,  1,  1,  1,  1,  1,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,\n         4,  4,  4,  4,  4,  4,  4,  4,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1]), 'orig_label': ['O', 'Name', 'Name', 'Name', 'Name', 'Name', 'Name', 'Designation', 'Designation', 'Designation', 'O', 'Companies worked at', 'Companies worked at', 'Location', 'Location', 'Location', 'O', 'O', 'O', 'O', 'O', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'Email Address', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Designation', 'Designation', 'Designation', 'Companies worked at', 'Companies worked at', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Designation', 'Designation', 'Designation', 'Designation', 'Designation', 'Designation', 'Designation', 'Designation', 'College Name', 'College Name', 'College Name', 'College Name', 'College Name', 'College Name', 'College Name', 'College Name', 'College Name', 'College Name', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Graduation Year', 'O', 'O', 'O', 'College Name', 'College Name', 'College Name', 'College Name', 'O', 'O', 'O', 'O', 'O', 'O', 'College Name', 'College Name', 'College Name', 'College Name', 'College Name', 'College Name', 'O', 'O', 'O', 'O', 'O', 'O', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'Skills', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc859c6189724a4b9410dc4f71f4a64a"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}]},{"cell_type":"code","source":"print(\"Training and Validating...\")\ntrain_and_val_model(\n    model,\n    TOKENIZER,\n    optimizer,\n    EPOCHS,\n    idx2tag,\n    tag2idx,\n    MAX_GRAD_NORM,\n    DEVICE,\n    train_dl,\n    val_dl\n)\n\ntorch.save(\n    {\n        \"model_state_dict\": model.state_dict()\n    },\n    f'{output_path}/model-state.bin',\n)\n","metadata":{"execution":{"iopub.status.busy":"2024-10-24T06:31:42.385871Z","iopub.execute_input":"2024-10-24T06:31:42.386361Z","iopub.status.idle":"2024-10-24T07:14:02.258693Z","shell.execute_reply.started":"2024-10-24T06:31:42.386325Z","shell.execute_reply":"2024-10-24T07:14:02.257242Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Training and Validating...\n","output_type":"stream"},{"name":"stderr","text":"Epoch:   0%|          | 0/5 [00:00<?, ?it/s]","output_type":"stream"},{"name":"stdout","text":"Starting training loop.\nTrain loss: 0.8714336286420408\nTrain accuracy: 0.7829989026064017\nStarting validation loop.\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  20%|██        | 1/5 [08:25<33:43, 505.90s/it]","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.6161925315856933\nValidation Accuracy: 0.819594383775351\nClassification Report:\n                     precision    recall  f1-score   support\n\n               ame       0.00      0.00      0.00        41\nears of Experience       0.00      0.00      0.00         5\n             egree       0.00      0.00      0.00        35\n        esignation       0.00      0.00      0.00        89\n             kills       0.18      0.02      0.03       869\n      mail Address       0.56      0.53      0.55      1130\n           ocation       0.00      0.00      0.00        54\n       ollege Name       0.00      0.00      0.00        33\nompanies worked at       0.00      0.00      0.00        59\n    raduation Year       0.00      0.00      0.00        16\n\n         micro avg       0.53      0.26      0.35      2331\n         macro avg       0.07      0.05      0.06      2331\n      weighted avg       0.34      0.26      0.28      2331\n\nConfusion Matrix:\n \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\nCollege Name\t\t\t[  0   0   0   0  11   0   0   0 189   8   0]\nCompanies worked at\t\t\t[  0   0   0   0  50   0   0   0 179   7   0]\nDegree\t\t\t[  0   0   0   0   0   0   0   0 148   4   0]\nDesignation\t\t\t[  0   0   0   0  35   0   0   0 280   8   0]\nEmail Address\t\t\t[  0   0   0   0 600   0   0   0 523   7   0]\nGraduation Year\t\t\t[ 0  0  0  0  0  0  0  0 19  0  0]\nLocation\t\t\t[  0   0   0   0  56   0   0   0 114   5   0]\nName\t\t\t[  0   0   0   0 150   0   0   0  39   1   0]\nO\t\t\t[    0     0     0     0   157     0     0     0 12518    33     0]\nSkills\t\t\t[  0   0   0   0   1   0   0   0 852  16   0]\nYears of Experience\t\t\t[ 0  0  0  0  2  0  0  0 12  1  0]\nStarting training loop.\nTrain loss: 0.4702043883178545\nTrain accuracy: 0.8428985934557076\nStarting validation loop.\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  40%|████      | 2/5 [17:00<25:32, 510.94s/it]","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.4149083703756332\nValidation Accuracy: 0.8514196567862714\nClassification Report:\n                     precision    recall  f1-score   support\n\n               ame       0.11      0.20      0.14        41\nears of Experience       0.00      0.00      0.00         5\n             egree       0.00      0.00      0.00        35\n        esignation       0.00      0.00      0.00        92\n             kills       0.67      0.33      0.44       869\n      mail Address       0.64      0.81      0.72      1130\n           ocation       0.00      0.00      0.00        53\n       ollege Name       0.00      0.00      0.00        33\nompanies worked at       0.00      0.00      0.00        60\n    raduation Year       0.00      0.00      0.00        16\n\n         micro avg       0.61      0.52      0.56      2334\n         macro avg       0.14      0.13      0.13      2334\n      weighted avg       0.56      0.52      0.51      2334\n\nConfusion Matrix:\n \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\nCollege Name\t\t\t[ 21   0   0   0   2   0   1   2 180   2   0]\nCompanies worked at\t\t\t[  1   3   0   0  39   0   2  11 187   2   0]\nDegree\t\t\t[ 13   1   2   1  13   0   0   1 107  14   0]\nDesignation\t\t\t[  0   0   0   5  49   0   3   9 238  10   0]\nEmail Address\t\t\t[  0   0   0   0 918   0   0   0 212   0   0]\nGraduation Year\t\t\t[ 0  0  0  0  0  0  0  0 19  0  0]\nLocation\t\t\t[  0   0   0   0  26   0  27  11 111   0   0]\nName\t\t\t[  0   0   0   0  56   0   2 124   8   0   0]\nO\t\t\t[    6     2     0     0   321     0     4     7 12258   110     0]\nSkills\t\t\t[  0   0   0   0   0   0   0   0 583 286   0]\nYears of Experience\t\t\t[ 0  0  0  0  3  0  0  0 11  1  0]\nStarting training loop.\nTrain loss: 0.32189113873502484\nTrain accuracy: 0.8911321795717065\nStarting validation loop.\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  60%|██████    | 3/5 [25:36<17:06, 513.27s/it]","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.3593557670712471\nValidation Accuracy: 0.8414976599063962\nClassification Report:\n                     precision    recall  f1-score   support\n\n               ame       0.25      0.46      0.32        41\nears of Experience       0.00      0.00      0.00         5\n             egree       0.10      0.20      0.14        35\n        esignation       0.17      0.21      0.19        89\n             kills       0.39      0.72      0.50       869\n      mail Address       0.85      0.75      0.80      1130\n           ocation       0.18      0.28      0.22        54\n       ollege Name       0.04      0.15      0.06        33\nompanies worked at       0.07      0.12      0.09        59\n    raduation Year       0.00      0.00      0.00        16\n\n         micro avg       0.48      0.67      0.56      2331\n         macro avg       0.21      0.29      0.23      2331\n      weighted avg       0.58      0.67      0.60      2331\n\nConfusion Matrix:\n \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\nCollege Name\t\t\t[149   2   9   0   0   0   4   4  40   0   0]\nCompanies worked at\t\t\t[13 92  0 15  1  0  8 17 86  4  0]\nDegree\t\t\t[ 18   1 107   1   0   0   0   1  24   0   0]\nDesignation\t\t\t[ 11  23   2 152   1   0   3  19 104   8   0]\nEmail Address\t\t\t[  0   0   0   0 851   0   0   5 274   0   0]\nGraduation Year\t\t\t[ 0  0  0  0  0  0  0  0 19  0  0]\nLocation\t\t\t[10  0  0  0  0  0 85 37 43  0  0]\nName\t\t\t[  0   0   0   0   0   0   0 188   2   0   0]\nO\t\t\t[  128    59    47    60   144     0    37     9 11232   992     0]\nSkills\t\t\t[  2   0   7   2   1   0   0   0 228 629   0]\nYears of Experience\t\t\t[ 0  0  0  1  1  0  0  0 13  0  0]\nStarting training loop.\nTrain loss: 0.2294413939766262\nTrain accuracy: 0.91884778335588\nStarting validation loop.\n","output_type":"stream"},{"name":"stderr","text":"Epoch:  80%|████████  | 4/5 [34:00<08:29, 509.53s/it]","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.2779960721731186\nValidation Accuracy: 0.8925429017160686\nClassification Report:\n                     precision    recall  f1-score   support\n\n               ame       0.83      0.85      0.84        41\nears of Experience       0.00      0.00      0.00         5\n             egree       0.13      0.11      0.12        35\n        esignation       0.20      0.15      0.17        92\n             kills       0.84      0.48      0.61       869\n      mail Address       0.83      0.76      0.79      1130\n           ocation       0.40      0.40      0.40        53\n       ollege Name       0.06      0.12      0.08        33\nompanies worked at       0.10      0.13      0.12        60\n    raduation Year       0.00      0.00      0.00        16\n\n         micro avg       0.73      0.59      0.65      2334\n         macro avg       0.34      0.30      0.31      2334\n      weighted avg       0.75      0.59      0.65      2334\n\nConfusion Matrix:\n \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\nCollege Name\t\t\t[121   0   6   0   0   0   0   0  81   0   0]\nCompanies worked at\t\t\t[  2  68   0   2   0   0   0   3 170   0   0]\nDegree\t\t\t[12  0 86  1  0  0  0  0 53  0  0]\nDesignation\t\t\t[  0  21   0 134   0   0   0   0 159   0   0]\nEmail Address\t\t\t[  0   0   0   0 862   0   0   0 268   0   0]\nGraduation Year\t\t\t[ 0  0  0  0  0  0  0  0 19  0  0]\nLocation\t\t\t[ 1  0  0  0  0  0 98  2 74  0  0]\nName\t\t\t[  0   1   0   2   0   0   0 183   4   0   0]\nO\t\t\t[   50    30     6    13   179     0    21     0 12332    77     0]\nSkills\t\t\t[  0   1   0   0   0   0   0   0 449 419   0]\nYears of Experience\t\t\t[ 0  0  0  0  0  0  0  0 15  0  0]\nStarting training loop.\nTrain loss: 0.1671412846316462\nTrain accuracy: 0.9390447313379141\nStarting validation loop.\n","output_type":"stream"},{"name":"stderr","text":"Epoch: 100%|██████████| 5/5 [42:19<00:00, 507.85s/it]","output_type":"stream"},{"name":"stdout","text":"Validation loss: 0.30792873799800874\nValidation Accuracy: 0.8828705148205929\nClassification Report:\n                     precision    recall  f1-score   support\n\n               ame       0.86      0.90      0.88        41\nears of Experience       0.00      0.00      0.00         5\n             egree       0.41      0.34      0.38        35\n        esignation       0.21      0.22      0.22        89\n             kills       0.80      0.31      0.45       869\n      mail Address       0.87      0.62      0.73      1130\n           ocation       0.53      0.50      0.51        54\n       ollege Name       0.22      0.39      0.28        33\nompanies worked at       0.14      0.31      0.19        59\n    raduation Year       0.00      0.00      0.00        16\n\n         micro avg       0.71      0.47      0.57      2331\n         macro avg       0.40      0.36      0.36      2331\n      weighted avg       0.77      0.47      0.57      2331\n\nConfusion Matrix:\n \tCollege Name Companies worked at Degree Designation Email Address Graduation Year Location Name O Skills Years of Experience\nCollege Name\t\t\t[162   0   3   0   0   0   0   0  43   0   0]\nCompanies worked at\t\t\t[  5 127   0   8   0   0   1   2  93   0   0]\nDegree\t\t\t[ 12   1 110   1   0   0   0   0  28   0   0]\nDesignation\t\t\t[  0  37   0 187   0   0   0   1  97   1   0]\nEmail Address\t\t\t[  0   0   0   0 703   0   0   0 427   0   0]\nGraduation Year\t\t\t[ 0  0  1  0  0  0  0  0 18  0  0]\nLocation\t\t\t[  6   0   0   0   0   0 110   0  59   0   0]\nName\t\t\t[  0   1   0   2   0   0   0 183   4   0   0]\nO\t\t\t[   80    94     2    38   106     0    27     0 12294    67     0]\nSkills\t\t\t[  0   1   0   0   0   0   0   0 596 272   0]\nYears of Experience\t\t\t[ 0  0  0  2  0  0  0  0 13  0  0]\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"code","source":"checkpoint = torch.load(f'{output_path}/model-state.bin')\nmodel.load_state_dict(checkpoint['model_state_dict'])","metadata":{"execution":{"iopub.status.busy":"2024-10-24T07:14:02.260937Z","iopub.execute_input":"2024-10-24T07:14:02.261432Z","iopub.status.idle":"2024-10-24T07:14:02.486159Z","shell.execute_reply.started":"2024-10-24T07:14:02.261385Z","shell.execute_reply":"2024-10-24T07:14:02.485032Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stderr","text":"/tmp/ipykernel_30/2027259983.py:1: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  checkpoint = torch.load(f'{output_path}/model-state.bin')\n","output_type":"stream"},{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"<All keys matched successfully>"},"metadata":{}}]},{"cell_type":"code","source":"import shutil\n\nshutil.make_archive('/kaggle/working/model', 'zip', '/kaggle/working', 'model-state.bin')","metadata":{"execution":{"iopub.status.busy":"2024-10-24T07:14:02.487661Z","iopub.execute_input":"2024-10-24T07:14:02.488446Z","iopub.status.idle":"2024-10-24T07:14:31.491438Z","shell.execute_reply.started":"2024-10-24T07:14:02.488397Z","shell.execute_reply":"2024-10-24T07:14:31.490309Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"'/kaggle/working/model.zip'"},"metadata":{}}]},{"cell_type":"code","source":"torch.save(\n    {\n        \"model_state_dict\": model.state_dict()\n    },\n    f'{output_path}/model-state.pth', \n)","metadata":{"execution":{"iopub.status.busy":"2024-10-24T07:14:31.492634Z","iopub.execute_input":"2024-10-24T07:14:31.492968Z","iopub.status.idle":"2024-10-24T07:14:32.010797Z","shell.execute_reply.started":"2024-10-24T07:14:31.492930Z","shell.execute_reply":"2024-10-24T07:14:32.009646Z"},"trusted":true},"execution_count":21,"outputs":[]}]}